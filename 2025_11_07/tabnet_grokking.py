from pytorch_tabnet.tab_model import TabNetClassifier
from sdv.datasets.local import load_csvs
import torch
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import StratifiedKFold, train_test_split
import pandas as pd
import numpy as np
np.random.seed(42)
import scipy
import sys, os, time
from pathlib import Path
sys.path.append(os.path.dirname(os.path.abspath(__file__)) + '/../')
from functions.Validation import auc_brier_ece
from itertools import product
import matplotlib.pyplot as plt
import random
from pytorch_tabnet.callbacks import Callback

class PeriodicCheckpoint(Callback):
    """
    ë§¤ period(100) ì—í¬í¬ë§ˆë‹¤:
    1) í•´ë‹¹ period êµ¬ê°„ ë‚´ best ëª¨ë¸ 1ê°œ ì €ì¥
    2) period ë§ˆì§€ë§‰ ì—í¬í¬(100, 200, 300...) ëª¨ë¸ 1ê°œ ì €ì¥
    
    êµ¬ê°„ ë‚´ì—ì„œëŠ” ë©”ëª¨ë¦¬ì—ë§Œ best ì •ë³´ë¥¼ ë³´ê´€í•˜ê³ , ë””ìŠ¤í¬ ì €ì¥ì€ period ë°°ìˆ˜ë§ˆë‹¤ë§Œ ì‹¤í–‰
    """
    def __init__(self, save_dir, period=100, metric_name="valid_auc"):
        self.save_dir = save_dir
        self.period = period
        self.metric_name = metric_name
        os.makedirs(save_dir, exist_ok=True)
        
        # í˜„ì¬ ë¸”ë¡ ë‚´ best ì •ë³´ë¥¼ ë©”ëª¨ë¦¬ì—ë§Œ ë³´ê´€
        self.current_block = 1
        self.best_metric_in_block = -float("inf")
        self.best_model_in_block = None  # ëª¨ë¸ state dictë¥¼ ì„ì‹œ ì €ì¥
        self.best_epoch_in_block = None
        
        print(f"[PeriodicCheckpoint] Initialized")
        print(f"  - Checkpoint period: {period} epochs")
        print(f"  - Metric: {metric_name}")
        print(f"  - Save dir: {save_dir}\n")
    
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        current_metric = logs.get(self.metric_name, None)
        
        if current_metric is None:
            return
        
        actual_epoch = epoch + 1  # TabNetì€ 0ë¶€í„° ì‹œì‘
        current_block = ((epoch) // self.period) + 1
        
        # ìƒˆë¡œìš´ ë¸”ë¡ ì‹œì‘ ì‹œ ì´ˆê¸°í™”
        if current_block != self.current_block:
            print(f"\n{'='*70}")
            print(f"[Block {current_block}] Starting epochs {(current_block-1)*self.period + 1} - {current_block*self.period}")
            print(f"{'='*70}")
            self.current_block = current_block
            self.best_metric_in_block = -float("inf")
            self.best_model_in_block = None
            self.best_epoch_in_block = None
        
        # ë¸”ë¡ ë‚´ best ì—…ë°ì´íŠ¸ (ë©”ëª¨ë¦¬ì—ë§Œ ì €ì¥)
        if current_metric > self.best_metric_in_block:
            self.best_metric_in_block = current_metric
            self.best_epoch_in_block = actual_epoch
            # ëª¨ë¸ì˜ state_dict ë³µì‚¬ë³¸ì„ ë©”ëª¨ë¦¬ì— ì €ì¥
            self.best_model_in_block = {
                'epoch': actual_epoch,
                'metric': current_metric,
                'network_state': self.trainer.network.state_dict().copy()
            }
            print(f"  [Block {self.current_block}] New best at epoch {actual_epoch}: {self.metric_name}={current_metric:.6f}")
        
        # period ë°°ìˆ˜ ì—í¬í¬ì— ë„ë‹¬í•˜ë©´ ë””ìŠ¤í¬ì— ì €ì¥
        if actual_epoch % self.period == 0:
            print(f"\n{'*'*70}")
            print(f"[SAVING] Reached epoch {actual_epoch} - Saving 2 models...")
            print(f"{'*'*70}")
            
            # 1) í˜„ì¬(period ë°°ìˆ˜) ì—í¬í¬ ëª¨ë¸ ì €ì¥
            checkpoint_path = os.path.join(
                self.save_dir,
                f"epoch{actual_epoch:04d}_checkpoint"
            )
            self.trainer.save_model(checkpoint_path)
            print(f"  âœ“ Saved checkpoint: epoch {actual_epoch}")
            
            # 2) ë¸”ë¡ ë‚´ best ëª¨ë¸ ì €ì¥
            if self.best_model_in_block is not None:
                best_path = os.path.join(
                    self.save_dir,
                    f"epoch{actual_epoch:04d}_block_best_epoch{self.best_epoch_in_block:04d}_auc{self.best_metric_in_block:.4f}"
                )
                
                # best ëª¨ë¸ì˜ state_dictë¥¼ í˜„ì¬ ëª¨ë¸ì— ë¡œë“œí•œ í›„ ì €ì¥
                current_state = self.trainer.network.state_dict().copy()  # í˜„ì¬ ìƒíƒœ ë°±ì—…
                self.trainer.network.load_state_dict(self.best_model_in_block['network_state'])
                self.trainer.save_model(best_path)
                self.trainer.network.load_state_dict(current_state)  # ì›ë˜ ìƒíƒœë¡œ ë³µì›
                
                print(f"  âœ“ Saved block best: epoch {self.best_epoch_in_block} (auc={self.best_metric_in_block:.6f})")
            
            print(f"{'*'*70}\n")
            
            # ë¸”ë¡ ìš”ì•½
            print(f"[Block {self.current_block} Summary]")
            print(f"  - Epochs: {(self.current_block-1)*self.period + 1} - {actual_epoch}")
            print(f"  - Best epoch: {self.best_epoch_in_block}")
            print(f"  - Best {self.metric_name}: {self.best_metric_in_block:.6f}\n")
    
    def on_train_end(self, logs=None):
        """í•™ìŠµ ì¢…ë£Œ ì‹œ ìµœì¢… ìš”ì•½"""
        print(f"\n{'='*70}")
        print(f"[Training Complete]")
        if self.best_epoch_in_block is not None:
            print(f"Last block ({self.current_block}) best: epoch {self.best_epoch_in_block}, auc={self.best_metric_in_block:.6f}")
        print(f"All checkpoints saved to: {self.save_dir}")
        print(f"{'='*70}\n")

def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)






# 1. í•™ìŠµ ê³¡ì„  ì‹œê°í™” ê°œì„  (ê·¸ë¡œí‚¹ í˜„ìƒ ê´€ì°°ìš©)
def plot_grokking_metrics(history, save_path=None):
    """ê·¸ë¡œí‚¹ í˜„ìƒì„ ê´€ì°°í•˜ê¸° ìœ„í•œ ìƒì„¸ ì‹œê°í™”"""
    import matplotlib.pyplot as plt
    import numpy as np
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1) Train/Valid Loss (ë¡œê·¸ ìŠ¤ì¼€ì¼)
    ax = axes[0, 0]
    ax.plot(history['loss'], label='Train Loss', alpha=0.7)
    if 'valid_loss' in history:
        ax.plot(history['valid_loss'], label='Valid Loss', alpha=0.7)
    ax.set_yscale('log')
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Loss (log scale)')
    ax.set_title('Loss over Time')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 2) Train/Valid AUC (ê·¸ë¡œí‚¹ ê´€ì°°ìš©)
    ax = axes[0, 1]
    ax.plot(history['train_auc'], label='Train AUC', alpha=0.7)
    ax.plot(history['valid_auc'], label='Valid AUC', alpha=0.7)
    ax.axhline(y=0.5, color='r', linestyle='--', alpha=0.3, label='Random')
    ax.set_xlabel('Epoch')
    ax.set_ylabel('AUC')
    ax.set_title('AUC over Time (Grokking Check)')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # ê·¸ë¡œí‚¹ íƒ€ì´ë° í‘œì‹œ
    if len(history['valid_auc']) > 10:
        valid_auc = np.array(history['valid_auc'])
        # Valid AUCê°€ ì²˜ìŒìœ¼ë¡œ 0.6ì„ ë„˜ëŠ” ì‹œì 
        grok_threshold = 0.6
        grok_epochs = np.where(valid_auc > grok_threshold)[0]
        if len(grok_epochs) > 0:
            first_grok = grok_epochs[0]
            ax.axvline(x=first_grok, color='g', linestyle='--', alpha=0.5, 
                      label=f'Grok at epoch {first_grok}')
            ax.legend()
    
    # 3) Generalization Gap (overfitting ì²´í¬)
    ax = axes[1, 0]
    gap = np.array(history['train_auc']) - np.array(history['valid_auc'])
    ax.plot(gap, color='purple', alpha=0.7)
    ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Train AUC - Valid AUC')
    ax.set_title('Generalization Gap (Overfitting Check)')
    ax.grid(True, alpha=0.3)
    
    # 4) Learning Rate
    ax = axes[1, 1]
    ax.plot(history['lr'], color='orange', alpha=0.7)
    ax.set_yscale('log')
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Learning Rate (log scale)')
    ax.set_title('Learning Rate Schedule')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"[Saved plot] {save_path}")
    
    plt.show()


# 2. ê·¸ë¡œí‚¹ ê°ì§€ ì½œë°±
class GrokkingDetector(Callback):
    """ê·¸ë¡œí‚¹ í˜„ìƒì„ ê°ì§€í•˜ê³  ê¸°ë¡í•˜ëŠ” ì½œë°±"""
    
    def __init__(self, threshold_improvement=0.05, window_size=50):
        """
        Args:
            threshold_improvement: Valid AUCì˜ ê¸‰ê²©í•œ ìƒìŠ¹ì„ ê°ì§€í•˜ëŠ” ì„ê³„ê°’
            window_size: ì´ë™ í‰ê·  ìœˆë„ìš° í¬ê¸°
        """
        self.threshold = threshold_improvement
        self.window_size = window_size
        self.valid_auc_history = []
        self.grokking_detected = False
        self.grokking_epoch = None
        
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        valid_auc = logs.get('valid_auc', None)
        
        if valid_auc is not None:
            self.valid_auc_history.append(valid_auc)
            
            # ì¶©ë¶„í•œ íˆìŠ¤í† ë¦¬ê°€ ìŒ“ì´ë©´ ê·¸ë¡œí‚¹ ì²´í¬
            if len(self.valid_auc_history) >= self.window_size and not self.grokking_detected:
                recent_avg = np.mean(self.valid_auc_history[-self.window_size:])
                old_avg = np.mean(self.valid_auc_history[-2*self.window_size:-self.window_size]) \
                         if len(self.valid_auc_history) >= 2*self.window_size else 0.5
                
                improvement = recent_avg - old_avg
                
                # ê¸‰ê²©í•œ ì„±ëŠ¥ í–¥ìƒ ê°ì§€
                if improvement > self.threshold:
                    self.grokking_detected = True
                    self.grokking_epoch = epoch + 1
                    print(f"\n{'='*60}")
                    print(f"ğŸ¯ GROKKING DETECTED at Epoch {self.grokking_epoch}!")
                    print(f"Valid AUC improved by {improvement:.4f} over last {self.window_size} epochs")
                    print(f"{'='*60}\n")


# 3. ê°œì„ ëœ TabNet íŒŒë¼ë¯¸í„° (ê·¸ë¡œí‚¹ ê´€ì°°ìš©)
def get_grokking_tabnet_params():
    """ê·¸ë¡œí‚¹ ì‹¤í—˜ì— ì í•©í•œ TabNet íŒŒë¼ë¯¸í„°"""
    import torch
    
    return {
        # ëª¨ë¸ ìš©ëŸ‰ (ê·¸ë¡œí‚¹ì€ ì¶©ë¶„í•œ ëª¨ë¸ ìš©ëŸ‰ì´ í•„ìš”)
        "n_d": 64,           # 48 -> 64ë¡œ ì¦ê°€
        "n_a": 64,           # 48 -> 64ë¡œ ì¦ê°€
        "n_steps": 5,
        "gamma": 1.5,        # 1.3 -> 1.5 (ì¢€ ë” aggressive attention)
        "n_independent": 2,
        "n_shared": 2,
        
        # ì •ê·œí™” (ê·¸ë¡œí‚¹ì€ ê°•í•œ ì •ê·œí™”ê°€ ë„ì›€ë¨)
        "lambda_sparse": 1e-3,
        
        # ì˜µí‹°ë§ˆì´ì € (ë‚®ì€ learning rate + weight decay)
        "optimizer_fn": torch.optim.AdamW,
        "optimizer_params": dict(
            lr=5e-4,           # 1e-3 -> 5e-4 (ë” ëŠë¦¬ê²Œ)
            weight_decay=0.05  # 0.02 -> 0.05 (ë” ê°•í•œ ì •ê·œí™”)
        ),
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ (patienceë¥¼ ê¸¸ê²Œ)
        "scheduler_params": {
            "factor": 0.5,     # 0.05 -> 0.5 (ëœ aggressive)
            "patience": 100,   # patience ì¶”ê°€
            "min_lr": 1e-6
        },
        "scheduler_fn": torch.optim.lr_scheduler.ReduceLROnPlateau,
        
        "mask_type": "entmax",
        "device_name": "cuda" if torch.cuda.is_available() else "cpu",
        "seed": 42,
    }


# 4. ì¡°ê¸° ì¢…ë£Œ ë¹„í™œì„±í™” í•¨ìˆ˜
def train_for_grokking(clf, X_train, y_train, X_valid, y_valid, 
                       max_epochs=2000, checkpoint_period=100,
                       checkpoint_dir="./checkpoints"):
    """
    ê·¸ë¡œí‚¹ ê´€ì°°ì„ ìœ„í•œ í•™ìŠµ (ì¡°ê¸° ì¢…ë£Œ ì—†ìŒ)
    """
    from pytorch_tabnet.callbacks import Callback
    
    # ì½œë°± ì„¤ì •
    periodic_cb = PeriodicCheckpoint(
        save_dir=checkpoint_dir,
        period=checkpoint_period,
        metric_name="valid_auc"
    )
    
    grokking_cb = GrokkingDetector(
        threshold_improvement=0.05,
        window_size=50
    )
    
    print(f"\n{'='*60}")
    print(f"Starting Grokking Experiment")
    print(f"Max Epochs: {max_epochs}")
    print(f"Checkpoint Period: {checkpoint_period}")
    print(f"Early Stopping: DISABLED (for grokking observation)")
    print(f"{'='*60}\n")
    
    clf.fit(
        X_train=X_train, 
        y_train=y_train,
        eval_set=[(X_train, y_train), (X_valid, y_valid)],
        eval_name=['train', 'valid'],
        eval_metric=['auc'],
        max_epochs=max_epochs,
        patience=max_epochs,  # ì‚¬ì‹¤ìƒ ì¡°ê¸° ì¢…ë£Œ ë¹„í™œì„±í™”
        batch_size=4096,
        virtual_batch_size=256,
        num_workers=0,
        weights=1,
        drop_last=False,
        augmentations=None,
        callbacks=[periodic_cb, grokking_cb],
    )
    
    return clf, grokking_cb










def main():

    curr_path = os.getcwd()
    parent_path = os.path.dirname(curr_path)
    DATA_DIR  = os.path.join(parent_path, "data")
    OUT_DIR   = os.path.join(curr_path, "output")
    PARAM_DIR = os.path.join(parent_path, "params")
    META_DIR = os.path.join(parent_path, "metadata")
    SYN_DIR = os.path.join(DATA_DIR, "syn_data")
    train_path = os.path.join(DATA_DIR, "train")
    trainA = os.path.join(train_path, "A.csv")
    processed_dir = os.path.join(DATA_DIR, "A_processed")
    Atrain_labels = os.path.join(DATA_DIR, "train.csv")
    ctgan_param_dir = os.path.join(PARAM_DIR, "ctgan_synthesizer.pkl")
    trainA_pos_meta_dir = os.path.join(META_DIR, "trainA_positive_metadata.json")










    datasets = load_csvs(
        folder_name=f'{processed_dir}\\',
        read_csv_parameters={
            'skipinitialspace': True,
            'encoding': 'utf-8-sig'
        })

    trainA_processed = datasets['trainA_processed_fast']
    Atrain_labels = pd.read_csv(Atrain_labels)

    A_labels = Atrain_labels.query("Test == 'A'").copy()

    trainA = pd.merge(
        A_labels, trainA_processed,
        on='Test_id', how='inner',
        validate='one_to_one', suffixes=('', '_proc')
    )


    # ë‘ Testê°€ ë™ì¼í•œì§€ í™•ì¸ í›„ í•˜ë‚˜ë§Œ ë‚¨ê¸°ê¸°
    assert (trainA['Test'] == trainA['Test_proc']).all()
    trainA = trainA.drop(columns=['Test_proc'])


    # --- 1) ë¶ˆí•„ìš”í•œ ì¹¼ëŸ¼ ì œê±° & X, y ë¶„ë¦¬ ---
    drop_cols = ['Test_id', 'Test']  # ëª¨ë¸ì— ë¶ˆí•„ìš”

    # print(trainA.columns)

    # print(trainA['Label'].unique())

    # # --- 2) Label ê°’ 0/1 ê°œìˆ˜ ì„¸ê¸° ---
    # label_counts = trainA['Label'].value_counts()
    # print("\n[Label ë¶„í¬]")
    # print(label_counts)




    trainA = trainA.drop(columns=drop_cols)
    # print(trainA.columns)




    syn_pos_list = [10000, 50000, 100000, 200000, 400000]
    ctgan_syn_pos_dict = {}
    for item in syn_pos_list:
        synthe_path = os.path.join(SYN_DIR, f"ctgan_syn_pos_{item}.csv")
        ctgan_syn_pos_dict[item] = pd.read_csv(synthe_path)

    # ì ‘ê·¼
    ctgan_syn_pos_10000 = ctgan_syn_pos_dict[10000]
    ctgan_syn_pos_50000 = ctgan_syn_pos_dict[50000]
    ctgan_syn_pos_100000 = ctgan_syn_pos_dict[100000]
    ctgan_syn_pos_200000 = ctgan_syn_pos_dict[200000]
    ctgan_syn_pos_400000 = ctgan_syn_pos_dict[400000]











    # 1) real ë°ì´í„°ì—ì„œ pos / neg ë¶„ë¦¬
    trainA_pos_real = trainA[trainA['Label'] == 1].copy()
    trainA_neg_real = trainA[trainA['Label'] == 0].copy()

    print("[real pos shape]:", trainA_pos_real.shape)
    print("[real neg shape]:", trainA_neg_real.shape)

    # 2) í•©ì„± pos ë°ì´í„°ë“¤ì— Label=1 ì»¬ëŸ¼ ì¶”ê°€
    ctgan_syn_pos_10000 = ctgan_syn_pos_10000.copy()
    ctgan_syn_pos_50000 = ctgan_syn_pos_50000.copy()
    ctgan_syn_pos_100000 = ctgan_syn_pos_100000.copy()
    ctgan_syn_pos_200000 = ctgan_syn_pos_200000.copy()
    ctgan_syn_pos_400000 = ctgan_syn_pos_400000.copy()

    for df in [ctgan_syn_pos_10000, ctgan_syn_pos_50000, ctgan_syn_pos_100000, ctgan_syn_pos_200000, ctgan_syn_pos_400000]:
        df['Label'] = 1  # ëª¨ë‘ ì–‘ì„± í´ë˜ìŠ¤

    print("[syn_pos_10000 shape]:", ctgan_syn_pos_10000.shape)
    print("[syn_pos_50000 shape]:", ctgan_syn_pos_50000.shape)
    print("[syn_pos_100000 shape]:", ctgan_syn_pos_100000.shape)
    print("[syn_pos_200000 shape]:", ctgan_syn_pos_200000.shape)
    print("[syn_pos_400000 shape]:", ctgan_syn_pos_400000.shape)

    # # (ì„ íƒ) í•œ ë²ˆì— ì“¸ í•©ì„± posë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê³  ì‹¶ë‹¤ë©´:
    syn_pos_all = pd.concat(
        [ctgan_syn_pos_10000, ctgan_syn_pos_50000, ctgan_syn_pos_100000],
        axis=0,
        ignore_index=True
    )
    print("[syn_pos_all shape]:", syn_pos_all.shape)
















    ####### CV #########################################################################################################



    # ---------------------------
    # 0) ë² ì´ìŠ¤ ë°ì´í„° ì¤€ë¹„
    # ---------------------------
    # trainA: real ì „ì²´ ë°ì´í„° (Label í¬í•¨)
    assert 'Label' in trainA.columns

    # í•©ì„± pos í•˜ë‚˜ ê³¨ë¼ì„œ ì‚¬ìš© (ì—¬ê¸°ì„œëŠ” 1ë§Œì§œë¦¬ ì˜ˆì‹œ)
    syn_pos = ctgan_syn_pos_200000.copy()
    syn_pos['Label'] = 1  # í˜¹ì‹œ ì•ˆ ë¶™ì–´ìˆë‹¤ë©´ í™•ì‹¤íˆ í•´ë‘ê¸°

    # real ì „ì²´ì—ì„œ X, y ë¶„ë¦¬
    X_real = trainA.drop(columns=['Label'])
    y_real = trainA['Label']

    print("X_real shape:", X_real.shape)
    print("y_real value counts:\n", y_real.value_counts())

    # í•©ì„± posì—ì„œë„ X, y ë¶„ë¦¬
    X_syn = syn_pos.drop(columns=['Label'])
    y_syn = syn_pos['Label']  # ì „ë¶€ 1ì´ì–´ì•¼ í•¨

    print("X_syn shape:", X_syn.shape)
    print("y_syn unique:", y_syn.unique())














    # 1. real ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ í†µì¼ëœ í†µê³„ ì‚¬ìš©
    real_median = X_real.median()

    # 2. real / syn / valid ëª¨ë‘ ê°™ì€ ê¸°ì¤€ìœ¼ë¡œ ì±„ì›€
    X_real = X_real.fillna(real_median)
    X_syn = X_syn.fillna(real_median)

    # 3. ì´í›„ train/valid split ì¬ì‹¤í–‰
    X_train_real, X_valid_real, y_train_real, y_valid_real = train_test_split(
        X_real, y_real, test_size=0.2, stratify=y_real, random_state=42
    )

    # 4. í•©ì„± pos ë¶™ì´ê¸°
    X_train_all = pd.concat([X_train_real, X_syn], ignore_index=True)
    y_train_all = pd.concat([y_train_real, y_syn], ignore_index=True)









    # print("== NaN check: real / syn / combined ==")
    # print("X_real NaN ê°œìˆ˜:", X_real.isna().sum().sum())
    # print("X_syn NaN ê°œìˆ˜:", X_syn.isna().sum().sum())

    # X_train_all = pd.concat([X_train_real, X_syn], axis=0, ignore_index=True)
    # print("X_train_all NaN ê°œìˆ˜:", X_train_all.isna().sum().sum())


    # nan_cols = X_train_all.columns[X_train_all.isna().any()]
    # print("NaN ìˆëŠ” ì»¬ëŸ¼:", nan_cols.tolist())
    # print(X_train_all[nan_cols].isna().sum())








    print("[real train] shape:", X_train_real.shape,
        "pos=", (y_train_real == 1).sum(),
        "neg=", (y_train_real == 0).sum())
    print("[real valid] shape:", X_valid_real.shape,
        "pos=", (y_valid_real == 1).sum(),
        "neg=", (y_valid_real == 0).sum())

    # ---------------------------
    # 1) trainì—ë§Œ í•©ì„± pos ë¶™ì´ê¸°
    # ---------------------------

    X_train_all = pd.concat([X_train_real, X_syn], ignore_index=True)
    y_train_all = pd.concat([y_train_real, y_syn], ignore_index=True)

    print("[train + synthetic] shape:", X_train_all.shape,
        "pos=", (y_train_all == 1).sum(),
        "neg=", (y_train_all == 0).sum())

    features = list(X_real.columns)  # ì „ì²´ í”¼ì²˜ ì´ë¦„

    X_train = X_train_all[features].values.astype(np.float32)
    y_train = y_train_all.values.astype(int)

    X_valid = X_valid_real[features].values.astype(np.float32)
    y_valid = y_valid_real.values.astype(int)

    print("X_train shape:", X_train.shape, "y_train pos=", (y_train == 1).sum())
    print("X_valid shape:", X_valid.shape, "y_valid pos=", (y_valid == 1).sum())


    tabnet_params = get_grokking_tabnet_params()
    clf = TabNetClassifier(**tabnet_params)


    num_epochs = 3000  # ê·¸ë¡œí‚¹ ë§›ë³´ê¸°ëŠ” 300ë¶€í„° ì‹œì‘í•˜ì—¬ ë‚˜ì¤‘ì— ëŠ˜ë ¤ë„ ë¨
    # patience = 50

    # ì›í•˜ë©´ TabNet ë‚´ë¶€ augmentationsë„ ì“¸ ìˆ˜ ìˆìŒ
    # aug = ClassificationSMOTE(p=0.2)
    aug = None  # ì¼ë‹¨ ë”

    checkpoint_dir = os.path.join(OUT_DIR, "tabnet_checkpoints")


    # 2) í•™ìŠµ
    clf, grokking_cb = train_for_grokking(
        clf, X_train, y_train, X_valid, y_valid,
        max_epochs=num_epochs,
        checkpoint_period=100,
        checkpoint_dir=checkpoint_dir
    )

    # 3) ì‹œê°í™”
    plot_path = os.path.join(OUT_DIR, "grokking_analysis.png")
    plot_grokking_metrics(clf.history, save_path=plot_path)
    
    # 4) ê·¸ë¡œí‚¹ ì •ë³´ ì¶œë ¥
    if grokking_cb.grokking_detected:
        print(f"\nâœ… Grokking occurred at epoch {grokking_cb.grokking_epoch}")
    else:
        print(f"\nâŒ No clear grokking detected within {num_epochs} epochs")    

    # plot losses
    plt.figure(figsize=(10,4))

    plt.subplot(1,3,1)
    plt.plot(clf.history['loss'])
    plt.title("Train Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")

    plt.subplot(1,3,2)
    plt.plot(clf.history['train_auc'], label="train_auc")
    plt.plot(clf.history['valid_auc'], label="valid_auc")
    plt.title("AUC")
    plt.xlabel("Epoch")
    plt.ylabel("AUC")
    plt.legend()

    plt.subplot(1,3,3)
    plt.plot(clf.history['lr'])
    plt.title("Learning Rate")
    plt.xlabel("Epoch")

    plt.tight_layout()
    plt.show()



    # í™•ë¥  ì˜ˆì¸¡
    preds_valid = clf.predict_proba(X_valid)
    valid_auc = roc_auc_score(y_score=preds_valid[:, 1], y_true=y_valid)
    print(f"TabNet valid AUC: {valid_auc:.6f}")

    from functions.Validation import auc_brier_ece

    answer_df = pd.DataFrame({
        "id": np.arange(len(y_valid)),
        "Label": y_valid.astype(int)
    })

    submission_df = pd.DataFrame({
        "id": np.arange(len(preds_valid)),
        "Label": preds_valid[:, 1].astype(float)
    })

    combined_score = auc_brier_ece(answer_df, submission_df)
    print(f"TabNet valid combined score: {combined_score:.6f}")


if __name__ == "__main__":
    set_seed(42)
    main()